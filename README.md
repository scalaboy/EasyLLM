# EasyLLM 
we want to train llm in easy type,
Our goal for this project is to make training large models simple, so that there are no difficult-to-train large models in the world.
## üìùIntroduction
Firstly, I would like to clarify that due to the smaller size of our model parameters, it is likely not on par with large models like GPT-4. Competing with them is also not our objective. Our goals are as follows:

1 To gradually master the general skills of training large models under extremely low-cost conditions.

2 To be able to quickly experiment with and evaluate the merits and drawbacks of different model structures.

3 To truly enter the field of large model algorithm development. 

In fact, if you find a good model structure that performs well with a small number of parameters and a small dataset, you can switch to a server with 10 cards of the H200 type, spend ten days, and work with a large dataset and a large model structure to train a model of around 10 billion parameters.



## DataSetüí°
## Train
## Test 
 




## Acknowledgement
1 https://github.com/charent/Phi2-mini-Chinese

2 https://github.com/DLLXW/baby-llama2-chinese

3 https://github.com/jiahe7ay/MINI_LLM





